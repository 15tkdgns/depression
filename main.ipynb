{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1fc53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "from preprocess.data_loader import load_data\n",
    "from preprocess.preprocessing import preprocess\n",
    "from preprocess.feature_selection import select_features\n",
    "from models.dnn import build_dnn\n",
    "from models.traditional import train_traditional_models\n",
    "from evaluations.evaluation import evaluate_model\n",
    "from configs import config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import os\n",
    "\n",
    "#오버샘플링 실험 함수 import\n",
    "from experiments.oversampling_comparison import (\n",
    "    oversampling_comparison, smote_k_neighbors_tuning\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- (1) 각 모델별 Optuna objective 함수 정의 -----------------\n",
    "\n",
    "def objective_dnn(trial, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Optuna로 DNN 하이퍼파라미터(은닉층/드롭아웃) 튜닝\n",
    "    \"\"\"\n",
    "    layers = trial.suggest_categorical(\n",
    "        'layers',\n",
    "        [(512, 256), (1024, 512, 256), (1024, 512, 256, 128)]\n",
    "    )\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    model = build_dnn(X_train.shape[1], layers, dropout=dropout, optimizer='Adam')\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=30, batch_size=32,\n",
    "        validation_split=0.2, verbose=0\n",
    "    )\n",
    "    return max(history.history['val_accuracy'])\n",
    "\n",
    "def objective_rf(trial, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Optuna로 RandomForest 하이퍼파라미터 튜닝\n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10)\n",
    "    }\n",
    "    rf = RandomForestClassifier(**params, random_state=config.SEED)\n",
    "    return cross_val_score(rf, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "def objective_xgb(trial, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Optuna로 XGBoost 하이퍼파라미터 튜닝\n",
    "    \"\"\"\n",
    "    import xgboost as xgb\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0)\n",
    "    }\n",
    "    clf = xgb.XGBClassifier(**params, random_state=config.SEED, use_label_encoder=False, eval_metric='mlogloss')\n",
    "    return cross_val_score(clf, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "def objective_lgbm(trial, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Optuna로 LightGBM 하이퍼파라미터 튜닝\n",
    "    \"\"\"\n",
    "    import lightgbm as lgb\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0)\n",
    "    }\n",
    "    clf = lgb.LGBMClassifier(**params, random_state=config.SEED)\n",
    "    return cross_val_score(clf, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "def objective_svm(trial, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Optuna로 SVM 하이퍼파라미터 튜닝\n",
    "    \"\"\"\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 0.1, 10.0, log=True),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['rbf', 'linear', 'poly']),\n",
    "        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "    }\n",
    "    clf = SVC(**params, probability=True, random_state=config.SEED)\n",
    "    return cross_val_score(clf, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "def objective_lr(trial, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Optuna로 LogisticRegression 하이퍼파라미터 튜닝\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 0.01, 10.0, log=True),\n",
    "        'solver': trial.suggest_categorical('solver', ['lbfgs', 'liblinear'])\n",
    "    }\n",
    "    clf = LogisticRegression(**params, max_iter=500, random_state=config.SEED)\n",
    "    return cross_val_score(clf, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "def objective_tabnet(trial, X_train, y_train, X_valid, y_valid):\n",
    "    \"\"\"\n",
    "    Optuna로 TabNet 하이퍼파라미터 튜닝\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'n_d': trial.suggest_int('n_d', 8, 64),\n",
    "        'n_a': trial.suggest_int('n_a', 8, 64),\n",
    "        'n_steps': trial.suggest_int('n_steps', 3, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1.0, 2.0),\n",
    "        'lambda_sparse': trial.suggest_float('lambda_sparse', 1e-5, 1e-1, log=True)\n",
    "    }\n",
    "    clf = TabNetClassifier(**params, seed=config.SEED)\n",
    "    clf.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        patience=10, max_epochs=100, batch_size=1024\n",
    "    )\n",
    "    preds = clf.predict(X_valid)\n",
    "    return evaluate_model(y_valid, preds)['accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- (2) 메인실행: 전체 파이프라인 + 오버샘플링 실험 추가 -----------------\n",
    "\n",
    "def main(search_time_minutes=5):\n",
    "    # === [1] 데이터 로딩/전처리 ===\n",
    "    df_original = load_data(config.DATA_PATH)\n",
    "    df, numeric_cols, categorical_cols = preprocess(df_original)\n",
    "    # 우울증 점수 그룹화: 0(정상), 1(경증), 2(중등도 이상)\n",
    "    df['mh_PHQ_S_grouped'] = df['mh_PHQ_S'].apply(lambda x: 0 if x <= 4 else 1 if x <= 9 else 2)\n",
    "    # 타겟/피처 분리\n",
    "    X = df.drop(['mh_PHQ_S', 'mh_PHQ_S_grouped'], axis=1)\n",
    "    y = df['mh_PHQ_S_grouped']\n",
    "    # 결측치 평균대치\n",
    "    X = pd.DataFrame(SimpleImputer(strategy='mean').fit_transform(X), columns=X.columns)\n",
    "    # 학습/평가 데이터 분리\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=config.SEED)\n",
    "    # 피처 선택 (미리 지정된 컬럼만)\n",
    "    X_train_selected, selected_features, selector = select_features(X_train, y_train, config.SELECTED_FEATURES)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    # 스케일러 (표준화)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "    # === [2] 오버샘플링 비교 실험 ===\n",
    "    print(\"\\n[실험] 오버샘플링 방법별 성능 비교 (Original vs SMOTE vs ADASYN vs Borderline-SMOTE)\")\n",
    "    df_os_results = oversampling_comparison(\n",
    "        X_train_scaled, y_train, X_test_scaled, y_test, config.SEED\n",
    "    )\n",
    "    os.makedirs(\"reports\", exist_ok=True)\n",
    "    df_os_results.to_excel('reports/oversampling_methods_comparison.xlsx', index=False)\n",
    "    print(\"[저장] 오버샘플링 성능 비교 결과 → reports/oversampling_methods_comparison.xlsx\")\n",
    "\n",
    "    # === [3] SMOTE k_neighbors 튜닝 실험 ===\n",
    "    print(\"\\n[실험] SMOTE의 k_neighbors 파라미터 튜닝\")\n",
    "    df_k_results = smote_k_neighbors_tuning(\n",
    "        X_train_scaled, y_train, X_test_scaled, y_test, config.SEED,\n",
    "        k_values=[3, 5, 7, 10, 15]\n",
    "    )\n",
    "    df_k_results.to_excel('reports/smote_k_neighbors_tuning.xlsx', index=False)\n",
    "    print(\"[저장] SMOTE k_neighbors 튜닝 결과 → reports/smote_k_neighbors_tuning.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbea030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # === [4] Optuna 하이퍼파라미터 탐색 ===\n",
    "    timeout = search_time_minutes * 1  # 단위: 초\n",
    "    model_objectives = [\n",
    "        ('DNN', objective_dnn),\n",
    "        ('RandomForest', objective_rf),\n",
    "        ('XGBoost', objective_xgb),\n",
    "        ('LightGBM', objective_lgbm),\n",
    "        ('SVM', objective_svm),\n",
    "        ('LogisticRegression', objective_lr),\n",
    "    ]\n",
    "    studies = {}\n",
    "    for model_name, objective in model_objectives:\n",
    "        print(f\"\\n[Optuna] {model_name} 하이퍼파라미터 튜닝 시작\")\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(lambda trial: objective(trial, X_train_scaled, y_train), timeout=timeout)\n",
    "        studies[model_name] = study\n",
    "        print(f\"[Optuna] {model_name} Best score: {study.best_value}, Best params: {study.best_params}\")\n",
    "    # TabNet만 별도\n",
    "    print(\"\\n[Optuna] TabNet 하이퍼파라미터 튜닝 시작\")\n",
    "    tabnet_study = optuna.create_study(direction='maximize')\n",
    "    tabnet_study.optimize(\n",
    "        lambda trial: objective_tabnet(\n",
    "            trial,\n",
    "            X_train_scaled.astype(np.float32), y_train.values,\n",
    "            X_test_scaled.astype(np.float32), y_test.values\n",
    "        ),\n",
    "        timeout=timeout\n",
    "    )\n",
    "    studies['TabNet'] = tabnet_study\n",
    "    print(f\"[Optuna] TabNet Best score: {tabnet_study.best_value}, Best params: {tabnet_study.best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a8e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # === [5] Best 파라미터로 모델별 재학습/성능 기록 ===\n",
    "    model_records = []\n",
    "    param_records = []\n",
    "    for model_name, study in studies.items():\n",
    "        best_params = study.best_params\n",
    "        print(f\"\\n[최종 모델 학습] {model_name} | best_params: {best_params}\")\n",
    "        if model_name == 'DNN':\n",
    "            model = build_dnn(\n",
    "                X_train_scaled.shape[1],\n",
    "                best_params['layers'],\n",
    "                dropout=best_params['dropout']\n",
    "            )\n",
    "            model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "            y_pred = model.predict(X_test_scaled).argmax(axis=1)\n",
    "        elif model_name == 'RandomForest':\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            model = RandomForestClassifier(**best_params, random_state=config.SEED)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "        elif model_name == 'XGBoost':\n",
    "            import xgboost as xgb\n",
    "            model = xgb.XGBClassifier(**best_params, random_state=config.SEED, use_label_encoder=False, eval_metric='mlogloss')\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "        elif model_name == 'LightGBM':\n",
    "            import lightgbm as lgb\n",
    "            model = lgb.LGBMClassifier(**best_params, random_state=config.SEED)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "        elif model_name == 'SVM':\n",
    "            from sklearn.svm import SVC\n",
    "            model = SVC(**best_params, probability=True, random_state=config.SEED)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "        elif model_name == 'LogisticRegression':\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            model = LogisticRegression(**best_params, max_iter=500, random_state=config.SEED)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "        elif model_name == 'TabNet':\n",
    "            model = TabNetClassifier(**best_params, seed=config.SEED)\n",
    "            model.fit(X_train_scaled.astype(np.float32), y_train.values, max_epochs=100, batch_size=1024)\n",
    "            y_pred = model.predict(X_test_scaled.astype(np.float32))\n",
    "        else:\n",
    "            continue\n",
    "        metrics = evaluate_model(y_test, y_pred)\n",
    "        model_records.append({'Model': model_name, **metrics})\n",
    "        param_records.append({'Model': model_name, 'Hyperparameters': best_params})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b352a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # === [6] 결과 리포트 저장 ===\n",
    "    now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    os.makedirs(\"reports\", exist_ok=True)\n",
    "    excel_filename = f\"reports/{now}_detailed_model_report.xlsx\"\n",
    "    with pd.ExcelWriter(excel_filename, engine='xlsxwriter') as writer:\n",
    "        pd.DataFrame(model_records).to_excel(writer, sheet_name='Performance', index=False)\n",
    "        pd.DataFrame(param_records).to_excel(writer, sheet_name='Hyperparameters', index=False)\n",
    "    print(f\"\\n모든 모델 결과가 '{excel_filename}'에 저장되었습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(search_time_minutes=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
